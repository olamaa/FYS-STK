{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1a)</b> <br/>\n",
    "We have the expression $f(x) = \\mathbf{a^Tx}$ where $a$ and $x$ are column vectors of length $n$. $f(x)$ is therefore a scalar.<br/>\n",
    "$x$ has dimensions (n, 1)<br/>\n",
    "The result (gradient) inherit transposed $x$ dimentions, (1, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1b)</b> <br/>\n",
    "We write $\\mathbf{a^Tx} = \\sum_{j=0}^{n-1}a_{j}x_j$. The partial derivative with respect to $x_i$ is\n",
    "$$\\frac{\\partial}{\\partial x_i} \\left(\\sum_{j=0}^{n-1} a_jx_j\\right) = a_i$$\n",
    "Since only the $i = j$ term contributes, this holds for all components $i$, so the gradient is ($a_1, a_2, ..., a_n$) which is $\\mathbf{a}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 1c)</b> <br/>\n",
    "$\\mathbf{a^TAa} = \\sum_{j=0}^{n-1} \\sum_{i=0}^{n-1} a_jA_{ij}a_i$<br/>\n",
    "For $i=k$: $A_{kj}a_j$ contributes<br/>\n",
    "For $j=k$: $a_iA_{ik}$ contributes<br/>\n",
    "Taking $\\frac{\\partial}{\\partial a_k}$, we get<br/>\n",
    "$$\\frac{\\partial}{\\partial a_k}\\left(\\mathbf{a^TAa}\\right) = \\sum_{j=0}^{n-1}A_{kj}a_j + \\sum_{i=0}^{n-1}a_iA_{ik}$$\n",
    "The first sum is the $k$ th component of $(\\mathbf{A}\\boldsymbol{a})$, and the second sum is the $k$ th component of $(\\boldsymbol{a}^T \\mathbf{A})^T = (\\mathbf{A}^T \\boldsymbol{a})$.\n",
    "$$\\left(\\mathbf{Aa}\\right)^T + \\left(\\mathbf{A}^T\\mathbf{a}\\right)^T = \\mathbf{a}^T\\mathbf{A}^T + \\mathbf{a}^T\\mathbf{A} = \\mathbf{a}^T\\left(\\mathbf{A}+\\mathbf{A}^T\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2a)</b><br/>\n",
    "Taking the derivative of the error with respect to $\\boldsymbol{\\theta}$ and setting it to zero finds the minimum because we are dealing with a convex quadratic cost function. The squared error $||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}||^2$ has a single global minimum. Setting the gradient to zero gives the critical point, which for a convex function is the global minimum. In short, solving $\\nabla_{\\theta} ||y - X\\theta||^2 = 0$ gives the $\\boldsymbol{\\theta}$ that minimizes the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2b)</b><br/>\n",
    "If $\\mathbf{X}$ is square and invertible, the optimal solution is the one that gives zero error by exactly solving $\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{y}$. In that case, we can multiply both sides by $\\mathbf{X}^{-1}$ to get\n",
    "$$\\boldsymbol{\\theta} = \\mathbf{X}^{-1}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2c)</b><br/>\n",
    "$$f(s) = \\left( \\mathbf{x}-\\mathbf{As}\\right)^T\\left(\\mathbf{x}-\\mathbf{As}\\right)$$\n",
    "We differentiate $f(s)$ with respect to $\\boldsymbol{s}$. This expression is the squared norm of the error vector $\\boldsymbol{e} = \\boldsymbol{x} - \\mathbf{A}\\boldsymbol{s}$.<br/>\n",
    "$$\\frac{\\partial}{\\partial s}\\left(\\mathbf{e}^T\\mathbf{e}\\right) = 2\\mathbf{e}^T\\frac{\\partial e}{\\partial s}$$\n",
    "Here $\\boldsymbol{e} = \\boldsymbol{x} - \\mathbf{A}\\boldsymbol{s}$, so $\\frac{\\partial \\boldsymbol{e}}{\\partial \\boldsymbol{s}} = -\\mathbf{A}$\n",
    "$$\\frac{\\partial}{\\partial s}\\left(\\mathbf{e}^T\\mathbf{e}\\right) = 2\\mathbf{e}^T\\left(-\\mathbf{A}\\right) = -2\\left(\\mathbf{x} - \\mathbf{As}\\right)^T\\mathbf{A}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 2d)</b><br/>\n",
    "In OLS, the error vector is $\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\theta}$. Using the formula above, and substituting $\\mathbf{x}= \\mathbf{y}$, $\\mathbf{A}= \\mathbf{X}$ and $\\mathbf{s}= \\mathbf{\\theta}$, we get\n",
    "$$-2\\left(\\mathbf{y} - \\mathbf{X\\theta}\\right)^T\\mathbf{X}.$$\n",
    "Optimizing this gives\n",
    "$$\\left(\\mathbf{y} - \\mathbf{X\\theta}\\right)^T\\mathbf{X} = 0^T$$\n",
    "$$\\mathbf{X}^T\\left(\\mathbf{y} - \\mathbf{X\\theta}\\right) = 0$$\n",
    "$$\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{X\\theta} = 0$$\n",
    "$$\\mathbf{X}^T\\mathbf{y} = \\mathbf{X}^T\\mathbf{X\\theta}$$\n",
    "Assuming $\\mathbf{X}^T \\mathbf{X}$ is invertible, we multiply by $(\\mathbf{X}^T\\mathbf{X})^{-1}$ to get\n",
    "$$\\theta = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 3a)</b><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "income = np.array([116., 161., 167., 118., 172., 163., 179., 173., 162., \n",
    "                   116., 101., 176., 178., 172., 143., 135., 160., 101., 149., 125.])\n",
    "children = np.array([5, 3, 0, 4, 5, 3, 0, 4, 4, 3, 3, 5, 1, 0, 2, 3, 2, 1, 5, 4])\n",
    "spending = np.array([152., 141., 102., 136., 161., 129.,  99., 159., 160., \n",
    "                     107.,  98., 164., 121.,  93., 112., 127., 117.,  69., 156., 131.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((n, 3))\n",
    "X[:, 0] = 1\n",
    "X[:, 1] = income\n",
    "X[:, 2] = children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlling the feature matrix X:\n",
      "[[  1. 116.   5.]\n",
      " [  1. 161.   3.]\n",
      " [  1. 167.   0.]\n",
      " [  1. 118.   4.]\n",
      " [  1. 172.   5.]]\n",
      "Shape of X: (20, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Controlling the feature matrix X:\")\n",
    "print(X[:5])\n",
    "print(\"Shape of X:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 3b)</b><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal OLS parameters beta: [ 9.12808583  0.5119025  14.60743095]\n"
     ]
    }
   ],
   "source": [
    "def OLS_parameters(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "beta = OLS_parameters(X, spending)\n",
    "print(\"Optimal OLS parameters beta:\", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would then be </br>\n",
    "spending $\\approx$ 9.13 + 0.512 $\\cdot$ income + 14.61 $\\cdot$ children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4a)</b><br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
